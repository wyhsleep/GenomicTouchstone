_name_: hyena_lm
d_model: 128
n_layer: 2
d_inner: ${eval:4 * ${.d_model}}
vocab_size: 12
resid_dropout: 0.0
embed_dropout: 0.1
fused_mlp: False
fused_dropout_add_ln: False
checkpoint_mixer: False  # set true for memory reduction
checkpoint_mlp: False  # set true for memory reduction
residual_in_fp32: True
pad_vocab_size_multiple: 8
layer:
  _name_: hyena
  emb_dim: 5
  filter_order: 64
  local_order: 3
  l_max: ${eval:${dataset.max_length}+2}
  modulate: True
  w: 10
  lr: ${optimizer.lr}
  wd: 0.0
  lr_pos_emb: 0.0
